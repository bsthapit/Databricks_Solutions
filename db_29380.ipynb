{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing Necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.master(\"yarn\")\\\n",
    "        .config(\"spark.port.maxRetries\", 100)\\\n",
    "        .config(\"spark.executor.instances\", \"6\")\\\n",
    "        .config(\"spark.executor.cores\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"16G\")\\\n",
    "        .config(\"spark.driver.memory\", \"4G\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "        .config(\"spark.yarn.queue\", \"Low\")\\\n",
    "        .config(\"spark.port.maxRetries\", 100)\\\n",
    "        .appName(\"TestJoin\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| colA|colB|\n",
      "+-----+----+\n",
      "|  Bob|   1|\n",
      "|  Sue|   3|\n",
      "| Paul|   4|\n",
      "|Alice|   2|\n",
      "| Josh|   2|\n",
      "+-----+----+\n",
      "\n",
      "+------+----+----+\n",
      "|  colA|colB|colC|\n",
      "+------+----+----+\n",
      "|   Bob|   3| XYZ|\n",
      "|   Sue|   2| XYZ|\n",
      "|   Pam|   2| XYZ|\n",
      "|Arthur|   2| XYZ|\n",
      "|  Josh|   1| ABC|\n",
      "+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test dataframes\n",
    "\n",
    "list_1 = [('Bob', 1),('Sue', 3), ('Paul', 4),('Alice', 2),('Josh', 2)]\n",
    "df1 = spark.createDataFrame(list_1, ['colA', 'colB'])\n",
    "\n",
    "list_2 = [('Bob', 3,'XYZ'),('Sue', 2,'XYZ'), ('Pam', 2,'XYZ'),('Arthur', 2,'XYZ'), ('Josh',1,'ABC')]\n",
    "df2 = spark.createDataFrame(list_2, ['colA', 'colB', 'colC'])\n",
    "\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+--------+\n",
      "|  colA|colB|colC|df1_colB|\n",
      "+------+----+----+--------+\n",
      "|   Sue|   2| XYZ|       3|\n",
      "|  Josh|   1| ABC|       2|\n",
      "|Arthur|   2| XYZ|    null|\n",
      "|   Bob|   3| XYZ|       1|\n",
      "|   Pam|   2| XYZ|    null|\n",
      "+------+----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Temporary dataframe to join df1 and df2, update df1.colB to zero if null\n",
    "\n",
    "temp_df = df2.join(df1, \"colA\", how = \"left\").select(\"*\", df1.colB.alias(\"df1_colB\")).drop(df1.colB)\n",
    "# .fillna(0)\n",
    "\n",
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+\n",
      "|  colA|colB|colC|\n",
      "+------+----+----+\n",
      "|   Sue|   5| XYZ|\n",
      "|  Josh|   1| ABC|\n",
      "|Arthur|   2| XYZ|\n",
      "|   Bob|   3| XYZ|\n",
      "|   Pam|   2| XYZ|\n",
      "+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update df2 if following coditions satisfy\n",
    "# Set df2.colB = df1.colB + df2.colB if\n",
    "# 1) df2.colB is less than df1.colB\n",
    "# 2) where df1.colA = df2.colA\n",
    "# 3) df.colC = XYZ\n",
    "\n",
    "df2 = temp_df.withColumn(\"colB\", when((col(\"colB\") <= col(\"df1_colB\")) \\\n",
    "                                         & (temp_df.df1_colB.isNotNull()) \\\n",
    "                                         & (temp_df.colC == 'XYZ'), col(\"colB\") + col(\"df1_colB\")).otherwise(col(\"colB\"))) \\\n",
    "                                        .drop(\"df1_colB\")\n",
    "    \n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
